{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from crawler import scraper, parser, utils\n",
    "from main_crawler import Crawler, Output, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "now = datetime.now()\n",
    "\n",
    "input = Input(today, now)\n",
    "\n",
    "chromedriver_loc = input.get_chromedriver_loc()\n",
    "sources = input.get_sources()\n",
    "sources_out_of_order = input.get_sources_out_of_order()\n",
    "sources_elements = input.get_sources_elements()\n",
    "search_name, search_terms = input.get_search_terms()\n",
    "(date_init, date_end) = input.get_date_interval()\n",
    "\n",
    "output = Output(search_name)\n",
    "\n",
    "crawler = Crawler(chromedriver_loc,\n",
    "                    sources,\n",
    "                    sources_out_of_order,\n",
    "                    sources_elements,\n",
    "                    search_terms,\n",
    "                    date_init,\n",
    "                    date_end,\n",
    "                    today,\n",
    "                    now,\n",
    "                    output,\n",
    "                    headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = crawler.setup_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altaveu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.altaveu.com/actualitat/successos/turista-circulava-mes-doble-alcohol-permes-dona-fills-menors-cotxe_60635_102.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup = self.static_methods.get_soup(link)\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    print(f\"Cannot access {url} right now. Please try again later.\")\n",
    "    soup = False\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "None\n",
      "None\n",
      "<section class=\"c-related-article\"><figure class=\"c-related-article__fig\"><a class=\"c-related-article__imglink\" href=\"https://www.altaveu.com/actualitat/successos/detingut-en-intentar-entrar-pais-mica-cocaina-extasi_60598_102.html\" title=\"Detingut en intentar entrar al país amb una mica de cocaïna i èxtasi\"><picture><source data-srcset=\"https://www.altaveu.com/uploads/s1/12/51/52/9/policia-duana_9_130x83.jpeg\" media=\"(min-width:0)\"/><img alt=\"Un control policial andorrà al Baladrà.\" class=\"c-related-article__img\" data-src=\"https://www.altaveu.com/uploads/s1/12/51/52/3/policia-duana.jpeg\" title=\"Un control policial andorrà al Baladrà.\"/></picture></a>\n",
      "</figure><div class=\"c-related-article__wrapper\">\n",
      "<p class=\"c-related-article__avantitle\">Relacionat</p>\n",
      "<div class=\"c-related-article__title\">\n",
      "<a class=\"c-related-article__link\" href=\"https://www.altaveu.com/actualitat/successos/detingut-en-intentar-entrar-pais-mica-cocaina-extasi_60598_102.html\" title=\"Detingut en intentar entrar al país amb una mica de cocaïna i èxtasi\">Detingut en intentar entrar al país amb una mica de cocaïna i èxtasi</a>\n",
      "</div>\n",
      "</div>\n",
      "</section>\n",
      "None\n",
      "\n",
      "Circulava amb la dona i dos fills menors, de disset i tretze anys. Ell en té 42 hi havien vingut a passar el dia a Andorra, a fer turisme. De cop, a quarts de nou del vespre de dilluns, va picar contra un altre vehicle a l’encreuament de Nacions Unides amb François Mitterrand, a la zona del Clot d’Emprivat, a Escaldes. Arran de la topada hi va haver una discussió i un conat de baralla que va fer que altres ciutadans que passaven per l’indret activessin la policia.\n",
      "Van arribar a lloc un parell de patrulles del cos d’ordre. La situació tensa va desaparèixer amb certa rapidesa. Els policies, és clar, van practicar la prova d’alcoholèmia i el turista en qüestió, causant de l’accident de danys materials, va donar una taxa d’1,84, més del doble del màxim permès per evitar la detenció. En conseqüència va ser arrestat per un delicte contra la seguretat col·lectiva. \n",
      "És una de les quatre persones que han estat detingudes aquesta setmana per donar positiu en controls d’alcoholèmia. Dos homes més, de 66 i 45 anys, han estat detinguts els darrers dies amb taxes de 0,97 i 0,98 respectivament, i al quart, de 19 anys, se l’ha arrestat per positiu en drogues al volant.\n",
      "Aquesta setmana, la policia també ha detingut un altre turista, de 39 anys, com a presumpte autor d’un delicte contra la salut pública. L’home, com ja va avançar l’Altaveu, accedia al país per la frontera amb França amb 1,26 grams de cocaïna i una pastilla d’èxtasi. Finalment, un home de 33 anys i no resident va ser arrestat com a presumpte autor d’un delicte contra la funció pública per incomplir una ordre d’expulsió administrativa.\n"
     ]
    }
   ],
   "source": [
    "# subtitle, content = get_content(self, journal, soup)\n",
    "opening = \"\"\n",
    "if soup.find('div', class_=\"c-mainarticle__opening\"):\n",
    "    # In l'Altaveu, we have to first find the opening in case there's one, as that is part of the content of the article\n",
    "    opening = soup.find('div', class_=\"c-mainarticle__opening\").text\n",
    "#print(soup.find('div', class_=\"c-mainarticle__body\"))\n",
    "paragraphs = soup.find('div', class_=\"c-mainarticle__body\").find_all('p', recursive=False)\n",
    "print(len(paragraphs))\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph.section)\n",
    "    #print(f\"\\n\\nPARAGRAPH:\\n{paragraph}\")\n",
    "content = opening + '\\n'.join([par.text for par in paragraphs if not par.section])\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bondia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bondia.ad/societat/astrie-retreu-la-falta-d-inversio-i-manca-de-resultats-en-habitatge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    print(f\"Cannot access {url} right now. Please try again later.\")\n",
    "    soup = False\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "comments_col = soup.find('div', class_=\"col-span-4 pt-2\")\n",
    "comments = comments_col.find_all('div', class_=\"flex flex-col gap-2 bg-primary-200 py-4 px-10\")\n",
    "print(len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Que retreu!? Serà l'herència que ha deixat Astrie amb la seva leydi lentejuelas Marsol a la cartera.. no es pot retreure amb un Comú just començant a legislar sinó s'haurà de deixar treballar i recuperar els fons perduts que eu deixat!!!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[0].find_all('div')[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bondia.ad/passava-per-aqui/seixanta-euros-pot-ser-poc-per-a-un-europeu-al-vietnam-canvien-vides\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    print(f\"Cannot access {url} right now. Please try again later.\")\n",
    "    soup = False\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle-body my-5 text-lg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([par\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m par \u001b[38;5;129;01min\u001b[39;00m paragraphs])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "paragraphs = soup.find('div', class_=\"article-body my-5 text-lg\").div.find_all('p')\n",
    "content = '\\n'.join([par.text for par in paragraphs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
