{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from crawler import scraper, parser, utils\n",
    "from main_crawler import Crawler, Output, Input\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing results as -> data/20240928_stju_articles.csv\n",
      "Storing articles in -> data/articles/\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "now = datetime.now()\n",
    "\n",
    "input = Input(today, now)\n",
    "\n",
    "chromedriver_loc = input.get_chromedriver_loc()\n",
    "sources = input.get_sources()\n",
    "sources_out_of_order = input.get_sources_out_of_order()\n",
    "sources_elements = input.get_sources_elements()\n",
    "search_name, search_terms = input.get_search_terms()\n",
    "date_init = datetime.strptime('20240928', '%Y%m%d').date()\n",
    "date_end = datetime.now()\n",
    "\n",
    "output = Output(search_name)\n",
    "\n",
    "crawler = Crawler(chromedriver_loc,\n",
    "                    sources,\n",
    "                    sources_out_of_order,\n",
    "                    sources_elements,\n",
    "                    search_terms,\n",
    "                    date_init,\n",
    "                    date_end,\n",
    "                    today,\n",
    "                    now,\n",
    "                    output,\n",
    "                    headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buttons(journal: str) -> None:\n",
    "    # We click the cookies and notifications buttons in the event we just opened the webpage and there are cookies and\n",
    "    # notifications pop ups that would not allow us to see the information on the page\n",
    "    cookies_loc = crawler.sources_elements.loc[journal, 'cookies']\n",
    "    notifs_loc = crawler.sources_elements.loc[journal, 'notifs']\n",
    "    # If the button hasn't been already pressed, journal is in cookies (meaning there is a button to press)\n",
    "    # and the button is there:\n",
    "    if not False and cookies_loc != '-' and driver.find_elements(By.XPATH, cookies_loc):\n",
    "        cookies_accept = driver.find_element(By.XPATH, cookies_loc)\n",
    "        driver.execute_script(\"arguments[0].click();\", cookies_accept)\n",
    "        crawler.cookies_clicked = True\n",
    "\n",
    "    if not False and notifs_loc != '-' and driver.find_elements(By.XPATH, notifs_loc):\n",
    "        notifications = driver.find_element(By.XPATH, notifs_loc)\n",
    "        driver.execute_script(\"arguments[0].click();\", notifications)\n",
    "        crawler.notifs_clicked = True\n",
    "\n",
    "def opening_url_actions(journal: str) -> None:  # journal specific !!\n",
    "    # Each journal has some actions we need to do after opening the url with the driver (sortin, adverts, etc.)\n",
    "\n",
    "    match journal:\n",
    "        case 'periodic':\n",
    "            # If there is an advertisement pop_up that stops us from accessing the journal, we click the \"Access journal\"\n",
    "            # button to access the journal\n",
    "            access = True\n",
    "            try:\n",
    "                access_journal = WebDriverWait(driver, 15).until(EC.presence_of_element_located((\n",
    "                    By.XPATH, '//div[@class=\"interstitial__link\"]/a')))\n",
    "            except TimeoutException:\n",
    "                access = False\n",
    "\n",
    "            if access:\n",
    "                driver.execute_script(\"arguments[0].click();\", access_journal)\n",
    "                driver.implicitly_wait(15)\n",
    "\n",
    "            buttons(journal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altaveu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.altaveu.com/actualitat/successos/turista-circulava-mes-doble-alcohol-permes-dona-fills-menors-cotxe_60635_102.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup = self.static_methods.get_soup(link)\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    print(f\"Cannot access {url} right now. Please try again later.\")\n",
    "    soup = False\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtitle, content = get_content(self, journal, soup)\n",
    "opening = \"\"\n",
    "if soup.find('div', class_=\"c-mainarticle__opening\"):\n",
    "    # In l'Altaveu, we have to first find the opening in case there's one, as that is part of the content of the article\n",
    "    opening = soup.find('div', class_=\"c-mainarticle__opening\").text\n",
    "#print(soup.find('div', class_=\"c-mainarticle__body\"))\n",
    "paragraphs = soup.find('div', class_=\"c-mainarticle__body\").find_all('p', recursive=False)\n",
    "print(len(paragraphs))\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph.section)\n",
    "    #print(f\"\\n\\nPARAGRAPH:\\n{paragraph}\")\n",
    "content = opening + '\\n'.join([par.text for par in paragraphs if not par.section])\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bondia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bondia.ad/societat/astrie-retreu-la-falta-d-inversio-i-manca-de-resultats-en-habitatge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    print(f\"Cannot access {url} right now. Please try again later.\")\n",
    "    soup = False\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_col = soup.find('div', class_=\"col-span-4 pt-2\")\n",
    "comments = comments_col.find_all('div', class_=\"flex flex-col gap-2 bg-primary-200 py-4 px-10\")\n",
    "print(len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[0].find_all('div')[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bondia.ad/passava-per-aqui/seixanta-euros-pot-ser-poc-per-a-un-europeu-al-vietnam-canvien-vides\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    print(f\"Cannot access {url} right now. Please try again later.\")\n",
    "    soup = False\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = soup.find('div', class_=\"article-body my-5 text-lg\").div.find_all('p')\n",
    "content = '\\n'.join([par.text for par in paragraphs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = crawler.setup_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.diariandorra.ad/nacional/240928/els-contrabandistes-esquiven-llei-trossejant-les-compres_158929.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_host = driver.find_element(By.CSS_SELECTOR, \"hyvor-talk-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use JavaScript to extract the shadow root's inner HTML\n",
    "shadow_dom_html = driver.execute_script(\"\"\"\n",
    "    // Access the shadow root and return its inner HTML\n",
    "    return arguments[0].shadowRoot.innerHTML;\n",
    "\"\"\", shadow_host)\n",
    "\n",
    "# Create a BeautifulSoup object with the extracted shadow DOM HTML\n",
    "soup = BeautifulSoup(shadow_dom_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = soup.find_all('div', class_=\"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19107408'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[2].find_all('div', class_=\"comment-meta-left-2\")[0].a['href'].split('comment-id=')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = comments[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19112152'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.find_all('div', class_=\"comment-meta-left-2\")[0].a['href'].split('comment-id=')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9/28/2024, 11:17:11 AM'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.find_all('time')[0]['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 9, 28, 11, 17, 11)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = '9/28/2024, 11:17:11 AM'\n",
    "datetime.strptime(date, \"%m/%d/%Y, %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Venuts'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.find_all('span', class_=\"user-name\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Era així fins que al 99 ens van entregar per salvar lo seu.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.find_all('div', class_=\"comment-content\")[0].div.p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment-replies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply = comment.parent.parent\n",
    "print(reply.get('class', ['-'])[0])\n",
    "reply.get('class', ['-'])[0] == \"comment-replies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19107408'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_comment = reply.parent\n",
    "parent_comment.find_all('div', class_=\"comment-meta-left-2\")[0].a['href'].split('comment-id=')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# periodic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing static?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.elperiodic.ad/noticia/108671/cardelus-sortira-en-22a-posicio-a-indonesia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "except:\n",
    "    print(f\"Cannot access {url} right now. Please try again later.\")\n",
    "    soup = False\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bondia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
